\subsubsection{Punto de partida}

El avance vertiginoso de Internet y la proliferación de servicios en línea han traído consigo un aumento significativo en la cantidad y sofisticación de las amenazas cibernéticas. Entre estas amenazas, el uso de \glspl{urls} maliciosas se ha convertido en una de las tácticas más prevalentes empleadas por los atacantes para comprometer sistemas y robar información. Las \glspl{urls} maliciosas son enlaces diseñados para engañar a los usuarios y llevarlos a sitios web fraudulentos, descargar software malicioso o realizar otras actividades dañinas. Estos enlaces se distribuyen a través de correos electrónicos de \gls{phishing}, redes sociales, mensajes instantáneos y otros medios de comunicación en línea.

Dada la magnitud del problema, se han desarrollado diversas técnicas y herramientas para la detección y mitigación de \glspl{urls} maliciosas. Sin embargo, la constante evolución de las tácticas de los atacantes hace necesario mejorar continuamente estos mecanismos de detección. Los métodos tradicionales, basados en listas negras y firmas, se ven rápidamente superados por la creación de nuevas \glspl{urls} maliciosas. En este contexto, el uso de técnicas avanzadas de análisis y aprendizaje automático (\textit{machine learning}) ofrece una solución prometedora para mejorar la detección y clasificación de \glspl{urls} maliciosas de manera más efectiva.

\subsubsection{Aportación realizada}

El presente trabajo tiene como objetivo principal desarrollar un sistema avanzado para la extracción y análisis de características de \glspl{urls}, que permita identificar de manera precisa \glspl{urls} maliciosas y benignas. La aportación principal de este proyecto se centra en la integración de múltiples fuentes de datos y técnicas de análisis para crear un conjunto de características robusto y detallado. Estas características incluyen, pero no se limitan a, aspectos léxicos de la \gls{url}, información \gls{whois}, datos geográficos de los servidores y la presencia de patrones sospechosos en los componentes de la \gls{url}.

Una de las innovaciones clave de este trabajo es la creación de un extractor de características de \glspl{urls} basado en una combinación de técnicas de procesamiento de texto, consultas a bases de datos y análisis de redes. Este extractor es capaz de manejar grandes volúmenes de datos y proporcionar información detallada sobre cada \gls{url}, lo que mejora significativamente la precisión de los modelos de clasificación y detección de amenazas.

El sistema desarrollado permite analizar la estructura y el contenido de las \glspl{url} para detectar la presencia de patrones sospechosos, como la inclusión de código hexadecimal o la utilización de dominios acortados y redireccionados, estrategias comúnmente empleadas por los atacantes para ocultar la verdadera naturaleza de sus enlaces. Esta capacidad de análisis avanzado no solo mejora la detección de \glspl{url} maliciosas, sino que también contribuye a la reducción de falsos positivos.

El objetivo final de este proyecto es entrenar un modelo de aprendizaje automático (\textit{machine learning}) utilizando las características extraídas de las \glspl{urls}, implementarlo en un entorno de tiempo real y desarrollar un \gls{dashboard} interactivo. Este \gls{dashboard} permitirá visualizar estadísticas como el número de \glspl{urls} benignas y maliciosas, las zonas geográficas de origen de las amenazas, y otras métricas relevantes para la ciberseguridad.

En resumen, la aportación de este trabajo radica en la combinación de técnicas avanzadas de análisis de \glspl{urls} con un enfoque práctico y escalable, que puede ser aplicado en entornos reales para mejorar la ciberseguridad y proteger a los usuarios de amenazas en línea.
